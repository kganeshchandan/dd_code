{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eaae5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "from random import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.gat import GATNet\n",
    "from models.gat_gcn import GAT_GCN\n",
    "from models.gcn import GCNNet\n",
    "from models.ginconv import GINConvNet\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "692ab7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function at each epoch\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    print('Training on {} samples...'.format(len(train_loader.dataset)))\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, data.y.view(-1, 1).float().to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n",
    "                                                                           batch_idx * len(data.x),\n",
    "                                                                           len(train_loader.dataset),\n",
    "                                                                           100. * batch_idx / len(train_loader),\n",
    "                                                                           loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65a927f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicting(model, device, loader):\n",
    "    model.eval()\n",
    "    total_preds = torch.Tensor()\n",
    "    total_labels = torch.Tensor()\n",
    "    print('Make prediction for {} samples...'.format(len(loader.dataset)))\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            total_preds = torch.cat((total_preds, output.cpu()), 0)\n",
    "            total_labels = torch.cat((total_labels, data.y.view(-1, 1).cpu()), 0)\n",
    "    return total_labels.numpy().flatten(),total_preds.numpy().flatten()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c03736c",
   "metadata": {},
   "source": [
    "# Hyper parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "52f495c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0005\n",
      "True\n",
      "Epochs:  1000\n"
     ]
    }
   ],
   "source": [
    "datasets, fold_id = ['davis'], '1'\n",
    "dataset=datasets[0]\n",
    "modeling = [GINConvNet, GATNet, GAT_GCN, GCNNet][0]\n",
    "model_st = modeling.__name__\n",
    "\n",
    "datasets\n",
    "\n",
    "cuda_name = \"cuda:0\"\n",
    "\n",
    "TRAIN_BATCH_SIZE = 512\n",
    "TEST_BATCH_SIZE = 512\n",
    "LR = 0.0005\n",
    "LOG_INTERVAL = 20\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "print('Learning rate: ', LR, flush=True)\n",
    "print(torch.cuda.is_available(), flush=True)\n",
    "print('Epochs: ', NUM_EPOCHS, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "418be479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nrunning on ', model_st + '_' + datasets[0] )\n",
    "# processed_data_file_train = 'data/processed/' + datasets[0] + '_train.pt'\n",
    "# processed_data_file_test = 'data/processed/' + datasets[0] + '_test.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "887ad7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atom_features(atom):\n",
    "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na','Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb','Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H','Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr','Cr', 'Pt', 'Hg', 'Pb', 'Unknown']) +\n",
    "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
    "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
    "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
    "                    [atom.GetIsAromatic()])\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def smile_to_graph(smile):\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    \n",
    "    c_size = mol.GetNumAtoms()\n",
    "    \n",
    "    features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        feature = atom_features(atom)\n",
    "        features.append( feature / sum(feature) )\n",
    "\n",
    "    edges = []\n",
    "    for bond in mol.GetBonds():\n",
    "        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
    "    g = nx.Graph(edges).to_directed()\n",
    "    edge_index = []\n",
    "    for e1, e2 in g.edges:\n",
    "        edge_index.append([e1, e2])\n",
    "        \n",
    "    return c_size, features, edge_index\n",
    "\n",
    "def seq_cat(prot):\n",
    "    x = np.zeros(max_seq_len)\n",
    "    for i, ch in enumerate(prot[:max_seq_len]): \n",
    "        x[i] = seq_dict[ch]\n",
    "    return x  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b74ea225",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_voc = \"ABCDEFGHIKLMNOPQRSTUVWXYZ\"\n",
    "seq_dict = {v:(i+1) for i,v in enumerate(seq_voc)}\n",
    "seq_dict_len = len(seq_dict)\n",
    "max_seq_len = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "174bf0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "df = pd.read_csv('data/' + dataset + '_train.csv')\n",
    "train_drugs, train_prots,  train_Y = list(df['compound_iso_smiles']),list(df['target_sequence']),list(df['affinity'])\n",
    "XT = [seq_cat(t) for t in train_prots]\n",
    "train_drugs, train_prots,  train_Y = np.asarray(train_drugs), np.asarray(XT), np.asarray(train_Y)\n",
    "\n",
    "\n",
    "# test set\n",
    "df = pd.read_csv('data/' + dataset + '_test.csv')\n",
    "test_drugs, test_prots,  test_Y = list(df['compound_iso_smiles']),list(df['target_sequence']),list(df['affinity'])\n",
    "XT = [seq_cat(t) for t in test_prots]\n",
    "test_drugs, test_prots,  test_Y = np.asarray(test_drugs), np.asarray(XT), np.asarray(test_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d2dac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "80bd8c2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'smile_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1686/1467787245.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestbedDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_drugs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_prots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msmile_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmile_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'smile_graph' is not defined"
     ]
    }
   ],
   "source": [
    "train_data = TestbedDataset(root='data', dataset=dataset+'_train', xd=train_drugs, xt=train_prots, y=train_Y,smile_graph=smile_graph)\n",
    "test_data = TestbedDataset(root='data', dataset=dataset+'_test', xd=test_drugs, xt=test_prots, y=test_Y,smile_graph=smile_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cde046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f822556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2af5ae90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processed data found: data/processed/davis_train.pt, loading ...\n",
      "Pre-processed data found: data/processed/davis_test.pt, loading ...\n"
     ]
    }
   ],
   "source": [
    "train_data = TestbedDataset(root='data', dataset=datasets[0]+'_train')\n",
    "test_data = TestbedDataset(root='data', dataset=datasets[0]+'_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46222051",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee60f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7848618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=TEST_BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4494352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch_geometric.data.dataloader.DataLoader at 0x7faae8f81d90>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52d47d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for batch_id, data in enumerate(train_loader):\n",
    "    print(batch_id)\n",
    "    x=data\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d4af53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(batch=[16950], c_size=[512], edge_index=[2, 37490], ptr=[513], target=[512, 1000], x=[16950, 78], y=[512])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc7f1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ca98ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
